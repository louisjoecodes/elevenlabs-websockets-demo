# How to Build a Real-Time Text-to-Speech Chatbot with Word Highlighting Using ElevenLabs WebSocket API

In this tutorial, we'll walk through creating a real-time text-to-speech (TTS) chatbot that highlights words as they are spoken. We'll leverage the ElevenLabs WebSocket API for streaming TTS, build a backend using Python and FastAPI, and connect it to a frontend built with Next.js.

If you're eager to jump straight into the code, you can find the complete repository [here](https://github.com/louisjoecodes/elevenlabs-websockets-demo).

## Introduction

Real-time text-to-speech applications have become increasingly popular in areas like accessibility tools, interactive learning platforms, and customer service bots. By highlighting words as they are spoken, users can follow along more easily, enhancing comprehension and engagement.

In this tutorial, we'll build a real-time TTS chatbot that not only converts text to speech but also provides word alignment data for highlighting. We'll use the ElevenLabs WebSocket API, which offers lower latency and real-time processing, making it ideal for generating dynamic audio responses to ChatGPT messages.

## Prerequisites

- Basic knowledge of Python and JavaScript
- An ElevenLabs account
- An OpenAI account

## Setting up the backend

We'll start by setting up the backend using Python with FastAPI and manage dependencies using Poetry.

### Initial Setup

1. **Create a New Project Directory**

```bash
mkdir elevenlabs-chatbot-backend
cd elevenlabs-chatbot-backend
```

2. **Initialize a New Poetry Project**

```bash
poetry init
```

When prompted, you can accept the defaults or provide specific details about your project.

3. **Update the `pyproject.toml` file**

Replace the contents of `pyproject.toml` with:

```toml
[tool.poetry]
name = "backend"
version = "0.1.0"
description = "Real-time TTS chatbot with word highlighting"
authors = ["Your Name <you@example.com>"]
readme = "README.md"
packages = [{ include = "src" }]

[tool.poetry.scripts]
start = "src.main:start"

[tool.poetry.dependencies]
python = "^3.12"
fastapi = "^0.115.0"        # Lightweight Web framework for building APIs
uvicorn = "^0.30.6"         # ASGI server for FastAPI
elevenlabs = "^1.9.0"       # ElevenLabs API client
python-dotenv = "^1.0.1"    # Load environment variables
websockets = "^13.0.1"      # WebSocket support
openai = "^1.46.0"          # OpenAI API client

[build-system]
requires = ["poetry-core"]
build-backend = "poetry.core.masonry.api"

```

4. **Create the Project Structure**

```bash
mkdir src
touch src/main.py
touch README.md
touch .env
```

Your directory structure should look like:

```bash
real-time-tts-chatbot/
├── src/
│   └── main.py
├── README.md
├── .env
├── pyproject.toml
```

5. **Set Up Environment Variables**

Update the `.env` file with your API keys (replace placeholders with your actual keys):

```bash
OPENAI_API_KEY=your_openai_api_key
ELEVENLABS_API_KEY=your_elevenlabs_api_key
```

**Note:** Keep your API keys secure and avoid committing the .env file to version control.

6. **Install dependencies**

```bash
poetry install
```

### Implementing the Chat Endpoint

Open `src/main.py` and start by importing necessary libraries and initializing the FastAPI app:

```python
import os
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware

# Load environment variables from .env file
load_dotenv()

app = FastAPI(
    title="Real-Time TTS Chatbot",
    description="Streams GPT-4 responses and converts them to speech with word highlighting using ElevenLabs TTS.",
    version="1.0.0"
)

# Allow all CORS origins (for demo purposes)
app.add_middleware(
    CORSMiddleware,
    allow_credentials=True,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

def start():
    uvicorn.run("src.main:app", host="0.0.0.0", port=8000, reload=True)

if __name__ == "__main__":
    start()
```

#### Streaming Responses from GPT-4

We'll use OpenAI's GPT-4 API to generate chat responses and stream the output to reduce latency.

```python
import asyncio
from openai import AsyncOpenAI

@app.post("/chat")
async def chat(request: Request):
    body = await request.json()
    messages = body.get('messages')

    chat_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    async def chat_response_stream():
        response = await chat_client.chat.completions.create(
            model='gpt-4',
            messages=messages,
            temperature=1,
            stream=True
        )
        async for chunk in response:
            delta = chunk.choices[0].delta
            content = delta.content
            if content:
                yield content

    # We'll build the text_chunker in the next step
    chat_response_chunks = text_chunker(chat_response_stream())

    # We'll build the elevenlabs_stream in the next step
    return StreamingResponse(elevenlabs_stream(chat_response_chunks), media_type="text/event-stream")
```

#### Connecting to the ElevenLabs WebSocket API

We'll stream the text output from GPT-4 to the ElevenLabs WebSocket API to get real-time audio and word alignment data.

**Define the ElevenLabs Stream Function**

```python
import websockets
import json

VOICE_ID = 'nPczCjzI2devNBz1zQrb' # Replace with your preferred ElevenLabs voice ID
VOICE_MODEL_ID = 'eleven_turbo_v2_5'
VOICE_SETTINGS = {
    "stability": 0.5,
    "similarity_boost": 0.8,
    "use_speaker_boost": False
}

async def elevenlabs_stream(text_iterator):
    uri = f"wss://api.elevenlabs.io/v1/text-to-speech/{VOICE_ID}/stream-input?model_id={VOICE_MODEL_ID}"

    async with websockets.connect(uri) as websocket:
        # Send initial message to establish connection
        await websocket.send(json.dumps({
            "text": " ",
            "voice_settings": VOICE_SETTINGS,
            "xi_api_key": os.getenv("ELEVENLABS_API_KEY"),
        }))

        queue = asyncio.Queue()

        async def send_text():
            async for text in text_iterator:
                await websocket.send(json.dumps({"text": text}))
            # Send a flush signal to indicate end of text
            await websocket.send(json.dumps({"text": " ", "flush": True}))

        async def receive_data():
            cumulative_run_time = 0
            received_final_chunk = False
            while not received_final_chunk:
                try:
                    message = await websocket.recv()
                    data = json.loads(message)
                    if data.get("audio"):
                        audio_data = data["audio"]
                        sse_message = f"data: {json.dumps({'type': 'audio', 'data': audio_data})}\n\n"
                        await queue.put(sse_message)
                    if data.get("alignment"):
                        alignment_info = data.get("alignment")
                        # We'll create the calculate_word_start_times function in the next step
                        words, word_start_times, cumulative_run_time = calculate_word_start_times(
                            alignment_info, cumulative_run_time
                        )
                        sse_message = f"data: {json.dumps({'type': 'word_times', 'data': {'words': words, 'wordStartTimesMs': word_start_times}})}\n\n"
                        await queue.put(sse_message)
                    if data.get('isFinal'):
                        received_final_chunk = True
                except websockets.exceptions.ConnectionClosed:
                    break
            await queue.put(None)  # Signal that we're done

        # Run send_text and receive_data concurrently
        send_task = asyncio.create_task(send_text())
        receive_task = asyncio.create_task(receive_data())

        # While data is available in the queue, yield it
        while True:
            data = await queue.get()
            if data is None:
                break
            yield data

        # Ensure all tasks are completed
        await send_task
        await receive_task

```

#### Handling Text Chunking

To ensure smooth TTS processing, we'll create a function that splits the incoming text into manageable chunks without breaking sentences.

```python
async def text_chunker(text_generator):
    splitters = (".", ",", "?", "!", ";", ":", "—", "-", "(", ")", "[", "]", "}", " ")
    buffer = ""

    async for text in text_generator:
        if buffer.endswith(splitters):
            yield buffer + " "
            buffer = text
        elif text.startswith(splitters):
            yield buffer + text[0] + " "
            buffer = text[1:]
        else:
            buffer += text

    if buffer:
        yield buffer + " "
```

#### Calculating Word Start Times

The WebSocket API response includes character alignment data, which we can use to calculate the start times of each word.

```python
def calculate_word_start_times(alignment_chunk, cumulative_run_time):
    # Adjust character start times to be cumulative
    adjusted_char_start_times = [time + cumulative_run_time for time in alignment_chunk['charStartTimesMs']]

    # Pair characters with their adjusted start times
    zipped_chars_times = list(zip(alignment_chunk['chars'], adjusted_char_start_times))

    # Find word boundaries and start times
    words = []
    word_start_times = []
    word = ''
    word_start_time = None
    for char, time in zipped_chars_times:
        if char == ' ':
            if word:
                words.append(word)
                word_start_times.append(word_start_time)
                word = ''
                word_start_time = None
        else:
            if not word:
                word_start_time = time
            word += char
    if word:
        words.append(word)
        word_start_times.append(word_start_time)

    # Calculate total duration of the chunk
    if 'charDurationsMs' in alignment_chunk and alignment_chunk['charDurationsMs']:
        total_duration = sum(alignment_chunk['charDurationsMs'])
    else:
        if adjusted_char_start_times:
            total_duration = adjusted_char_start_times[-1] - adjusted_char_start_times[0] + 100  # Added 100ms buffer
        else:
            total_duration = 0

    # Update cumulative_run_time
    cumulative_run_time += total_duration

    return words, word_start_times, cumulative_run_time
```

#### Running the Backend Server

To start the backend server, run:

```bash
poetry run start
```

#### Simulating the Output

To understand what the /chat route returns, let's simulate a sample interaction.

**Client sends**

```json
{
  "messages": [{ "role": "user", "content": "Tell me a joke." }]
}
```

**Server processes**

1. GPT-4 generates a joke, streamed as text chunks.

2. Individual words are sent to ElevenLabs via WebSocket.

3. ElevenLabs returns audio chunks and word alignment data.

**Client Receives Stream of Events**

```json
{
  "type": "audio",
  "data": "<base64-encoded-audio-chunk>"
}
{
  "type": "word_times",
  "data": {
    "words": ["Why", "don't", "scientists", "trust", "atoms?"],
    "wordStartTimesMs": [0, 150, 300, 450, 600]
  }
}
```

The frontend can now use the word start times to highlight the words as they are spoken!

## Setting Up the Frontend

To get the frontend running:

1. Clone or copy the repository and navigate into the frontend folder.

```bash
git clone https://github.com/louisjoecodes/elevenlabs-websockets-demo
cd elevenlabs-websockets-demo/frontend
```

2. Install the dependencies:

```bash
npm install
```

3. Start the development server:

```bash
npm run dev
```

Now you can access your app at http://localhost:3000 and test the real-time TTS chatbot with word highlighting.

### Front-End Handling

The frontend establishes a connection to the backend `/chat` endpoint and listens for streamed events. It synchronizes audio playback with word highlighting by using the wordStartTimesMs data to update the UI as each word is spoken.

## ElevenLabs WebSocket API Advanced Topics

### Buffering Mechanism

The ElevenLabs WebSocket service incorporates a buffering system designed to optimize the Time To First Byte (TTFB) while maintaining high-quality streaming. All text sent to the WebSocket endpoint is added to this buffer. Only when the buffer reaches a certain size is an audio generation attempted.

#### Why Buffering?

- **Quality Improvement:** The model provides higher-quality audio when it has longer inputs, as it can deduce more context about how the text should be delivered.
- **Latency Optimization:** Balancing between immediate audio generation and quality.

#### Generation Schedule

You can adjust the `chunk_length_schedule` in the `generation_config` to control how much text is buffered before generating audio. This allows you to optimize for lower latency or higher quality based on your application's needs.

**Example:**

```json
"generation_config": {
  "chunk_length_schedule": [120, 160, 250, 290]
}
```

- The first audio chunk will be generated after 120 characters.
- The second audio chunk will be generated after 160 characters etc.

**Trade-offs:**

- Lower Latency: Smaller chunk sizes mean audio is generated more frequently but may reduce quality.
- Higher Quality: Larger chunk sizes allow the model to gather more context, leading to better intonation and naturalness.

### Custom Voice Settings

You can fine-tune `VOICE_SETTINGS` to adjust the stability and similarity boost of the generated voice.

```python
VOICE_SETTINGS = {
    "stability": 0.75,        # Controls the consistency of the voice
    "similarity_boost": 0.85, # Controls how similar the voice is to the original sample
    "use_speaker_boost": True # Enhances the speaker's characteristics
}
```

Experiment with these settings to find the optimal balance for your application.

## Conclusion

Congratulations! You've built a real-time text-to-speech chatbot that streams both audio and word alignment data, allowing you to highlight spoken words in real time. This application leverages the power of OpenAI's GPT-4 for generating responses and ElevenLabs' WebSocket API for high-quality TTS conversion.

Feel free to explore and expand the project further. You might consider adding features like:

- **Voice Customization:** Allow users to select different voices or adjust voice settings dynamically.
- **Prompt Engineering:** Use [prompt techniques](https://elevenlabs.io/docs/speech-synthesis/prompting) to change the quality & delivery of the TTS output.

## Resources

- [Elevenlabs API Documentation](https://elevenlabs.io/docs/api-reference/getting-started)
- [OpenAI API Documentation](https://platform.openai.com/docs/api-reference/introduction)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Websockets in Python](https://docs.python.org/3/library/asyncio-protocol.html#websockets)
- [Next.js Documentation](https://nextjs.org/docs)
